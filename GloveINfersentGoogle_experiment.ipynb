{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Measure sentence similarity in a given dataset using Google's and Facebook's encoders \n",
    "\n",
    "It is a heavily modified version of nlp-town's <a href=\"https://github.com/nlptown/nlp-notebooks/blob/master/Simple%20Sentence%20Similarity.ipynb\"> notebook </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the input filenames. Feel free to leave them as they are\n",
    "- `INPUT_FILE_NAME`  is a file with students responses to be graded needs to be a .csv file with columns: IDStudent, IDStud, IDClass, Category, Field, Field_en, Accuracy_score, Code, Fieldname\n",
    "- `MODEL_FILE` is a file with model responses. It needs to have the following columns: 'TextName', 'Field1_en', 'Field2_en','Field3_en', 'Field4_en'. The TextName - first columns - needs to contian the name of the task corresponding to \"Category\" Column in the Input file. \n",
    "\n",
    "To be sure, look at the example datafiles provided and replace the content of the columns with the data from your experiment. Remember, that the responses of the students should not be empty. All texts should be in english.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_NAME = 'Example_dataset_marble_v2 - 2_data_no_omission.csv'\n",
    "MODEL_FILE = 'correct_answers.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the output filenames (if you change them, you will need to change them in B_ script as well, if you want to run postprocessing.)\n",
    "- `ALL_METHODS_RESULTS_FILE` will contain matched sentences from the model with their similarity scores\n",
    "- `SMOOTH_INVERSE_RESULTS_FILE` - part of the above file containing only SIF method matching and scores\n",
    "- `AVG_WORD2VEC_RESULTS_FILE` - same as above, only using the most primitive avarage of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_METHODS_RESULTS_FILE = \"GSE_INF_complete_result_matched.csv\"\n",
    "GSE_RESULTS_FILE = \"GSE_wv2_matched.csv\"\n",
    "infersent_RESULTS_FILE = \"INF_matched.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries\n",
    "Libraries ```seaborn```, ```tensorflow``` and ```tensorflow_hub``` are not included in the basic setup of the environment (requirements.txt) because of their size. At first you may need to install them. If you encounter \"module not found\" error, execute the lines below in terminal \n",
    "\n",
    "(activate the virtual environment first with `source env/bin/activate`)\n",
    "``\n",
    "pip intall seaborn\n",
    "pip install tensorflow\n",
    "pip install tensorflow_hub\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "STOP = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "class Sentence:\n",
    "    \n",
    "    def __init__(self, sentence):\n",
    "        self.raw = sentence\n",
    "        normalized_sentence = sentence.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        self.tokens = [t.lower() for t in nltk.word_tokenize(normalized_sentence)]\n",
    "        self.tokens_without_stop = [t for t in self.tokens if t not in STOP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InferSent\n",
    "\n",
    "[InferSent](https://github.com/facebookresearch/InferSent) is a pre-trained encoder that produces sentence embeddings. \n",
    "More particularly, it is a BiLSTM with max pooling that was trained on the SNLI dataset, 570k English sentence pairs labelled with one of three categories: entailment, contradiction or neutral. InferSent was developed and trained by Facebook Research.\n",
    "\n",
    "Let's first download the resources we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# !wget -nc https://raw.githubusercontent.com/facebookresearch/InferSent/master/models.py\n",
    "# !wget -nc https://s3.amazonaws.com/senteval/infersent/infersent.allnli.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first time you need to download the infersent1.pkl model. Uncomment and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: encoder: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  146M  100  146M    0     0  4366k      0  0:00:34  0:00:34 --:--:-- 6723k 0  3924k      0  0:00:38  0:00:28  0:00:10 5681k\n"
     ]
    }
   ],
   "source": [
    "# !mkdir encoder\n",
    "# !curl -Lo encoder/infersent1.pkl https://s3.amazonaws.com/senteval/infersent/infersent1.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infersent uses glove model. You can download it by uncommenting and running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -Lo models/ https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# !cd models && unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_GLOVE = os.path.expanduser(\"models/glove.840B.300d.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from models import InferSent\n",
    "V = 1\n",
    "MODEL_PATH = 'models/infersent1.pkl'\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "infersent.set_w2v_path(PATH_TO_GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # infersent = torch.load('infersent.allnli.pickle')\n",
    "\n",
    "# infersent = torch.load('infersent.allnli.pickle', map_location=lambda storage, loc: storage)\n",
    "# infersent.use_cuda = False\n",
    "# infersent.set_glove_path(PATH_TO_GLOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run the benchmark by having InferSent encode the two sets of sentences and compute the cosine similarity between the corresponding sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def run_inf_benchmark(sentences1, sentences2):\n",
    "    \n",
    "    raw_sentences1 = [sent1.raw for sent1 in sentences1]\n",
    "    raw_sentences2 = [sent2.raw for sent2 in sentences2]\n",
    "    \n",
    "    infersent.build_vocab(raw_sentences1 + raw_sentences2, tokenize=True)\n",
    "    embeddings1 = infersent.encode(raw_sentences1, tokenize=True)\n",
    "    embeddings2 = infersent.encode(raw_sentences2, tokenize=True)\n",
    "    \n",
    "    inf_sims = []\n",
    "    for (emb1, emb2) in zip(embeddings1, embeddings2): \n",
    "        sim = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
    "        inf_sims.append(sim)\n",
    "\n",
    "    return inf_sims   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Sentence Encoder\n",
    "\n",
    "The [Google Sentence Encoder](https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1) is Google's answer to Facebook's InferSent. It comes in two forms: \n",
    "\n",
    "- a Transformer model that takes the element-wise sum of the context-aware word representations produced by the encoding subgraph of a Transformer model.\n",
    "- a Deep Averaging Network (DAN) where input embeddings for words and bigrams are averaged together and passed through a feed-forward deep neural network.\n",
    "\n",
    "The Transformer model tends to give better results, but at the time of writing, only the DAN-based encoder was available.\n",
    "\n",
    "In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data (in a skip-thought-like task) and supervised data (the SNLI corpus).\n",
    "\n",
    "The Google Sentence Encoder can be loaded from the Tensorflow Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "#in case ssl errors appear\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like InferSent above, we'll have the it encode the two sets of sentences and return the similarities between the embeddings it produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gse_benchmark(sentences1, sentences2):\n",
    "    sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "    sts_input2 = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "    sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))\n",
    "    sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))\n",
    "        \n",
    "    sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "      \n",
    "        [gse_sims] = session.run(\n",
    "            [sim_scores],\n",
    "            feed_dict={\n",
    "                sts_input1: [sent1.raw for sent1 in sentences1],\n",
    "                sts_input2: [sent2.raw for sent2 in sentences2]\n",
    "            })\n",
    "    return gse_sims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Finally, it's time to run the actual experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "\n",
    "benchmarks = [\n",
    "              (\"GSE\", run_gse_benchmark),\n",
    "              (\"INF\", run_inf_benchmark)\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to our dataset\n",
    "First we open our input file, for example:\n",
    "\n",
    "`INPUT_FILE_NAME = 'Example_dataset_marble_v2 - 2_data_no_omission.csv'` \n",
    "- you can set the input file name at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(INPUT_FILE_NAME)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the sentences\n",
    "Declaring the main experiment function. The function below will run similarity measures for each responso of the student and check which sentence from the model is the most similar to students response. Thank it will match the most similar one and save the similarity score. This procedure takes place for all the similarity measures in `benchmarks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_match(df, model, benchmarks): \n",
    "    \"\"\"The function will run each of similarity measures in benchmarks \n",
    "    for each respons of the student and check which \n",
    "    sentence from the model is the most similar to student's \n",
    "    response. Than it will match the most similar model sentence \n",
    "    and save the similarity score. \"\"\"\n",
    "    size = len(model.index)\n",
    "    text_frame = df.copy()\n",
    "    sims = {\"stud_sentence\":[],\n",
    "            \"stud_field\":[],}\n",
    "    for label, method in benchmarks:\n",
    "        sims[label+\"_all_scores\"] = []\n",
    "        sims[label+\"_similarity\"] = []\n",
    "        sims[label+\"_aimed_sentence\"] = []\n",
    "        sims[label+\"_aimed_field\"] = []\n",
    "        \n",
    "    for index, row in text_frame.iterrows():\n",
    "        stud_sentence = row[\"Field_en\"]\n",
    "        sims[\"stud_sentence\"].append(stud_sentence)\n",
    "        sims[\"stud_field\"].append(row[\"Fieldname\"])\n",
    "        student_sentences = [Sentence(stud_sentence)]*size\n",
    "        model_sentences = model[row['Category']].apply(lambda s: Sentence(s))\n",
    "    #   pearson_cors, spearman_cors = [], []\n",
    "        for label, method in benchmarks:\n",
    "            similarity_scores = method(student_sentences, model_sentences)\n",
    "            similarity = max(similarity_scores)\n",
    "            index = np.argmax(similarity_scores)\n",
    "            aimed_sentence = model_sentences.iloc[index]\n",
    "            aimed_field = model_sentences.index[index]\n",
    "            sims[label+\"_all_scores\"].append(similarity_scores)\n",
    "            sims[label+\"_similarity\"].append(similarity)\n",
    "            sims[label+\"_aimed_sentence\"].append(aimed_sentence.raw)\n",
    "            sims[label+\"_aimed_field\"].append(aimed_field)\n",
    "    frame = pd.DataFrame(sims)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the model file and transposing it, so that text names become columns, and field_numbers become index rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_frame = pd.read_csv(MODEL_FILE, index_col=0)\n",
    "exp_frame2 = data.copy()\n",
    "model_frame = model_frame[[ 'Field1_en', 'Field2_en','Field3_en', 'Field4_en']]\n",
    "model_frame = model_frame.transpose()\n",
    "model_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment on the copy of the original\n",
    "frame_sim2 = run_all_match(exp_frame2, model_frame, benchmarks)\n",
    "frame_sim2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append exta columns to the experiments result: 'IDStud', 'IDClass', \"Category\",\"Accuracy_score\", \"Code\",\"Fieldname\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_sim2 = pd.concat([frame_sim2, \n",
    "                        exp_frame2[['IDStud', 'IDClass', \n",
    "                                    \"Category\",\"Accuracy_score\",\n",
    "                                    \"Code\",\"Fieldname\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the all the columns to `ALL_METHODS_RESULTS_FILE` and specific methods columns to other -`RESULTS_FILE`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_sim2.to_csv(ALL_METHODS_RESULTS_FILE)\n",
    "sif_matched = frame_sim2[[\"INF_aimed_sentence\", \n",
    "            \"INF_aimed_field\", \n",
    "            'INF_similarity',\n",
    "            'stud_field',\n",
    "            'stud_sentence',\n",
    "            'IDStud', 'IDClass', \n",
    "            \"Category\",\"Accuracy_score\",\n",
    "            \"Code\",\"Fieldname\"]].copy()\n",
    "sif_matched.to_csv(SMOOTH_INVERSE_RESULTS_FILE)\n",
    "avg_w2v_matched = frame_sim2[[\"GSE_aimed_sentence\", \n",
    "            \"GSE_aimed_field\", \n",
    "            'GSE_similarity',\n",
    "            'stud_field',\n",
    "            'stud_sentence',\n",
    "            'IDStud', 'IDClass', \n",
    "            \"Category\",\"Accuracy_score\",\n",
    "            \"Code\",\"Fieldname\"]].copy()\n",
    "avg_w2v_matched.to_csv(\"avg_wv2_matched.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_marble2",
   "language": "python",
   "name": "python3_marble2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
