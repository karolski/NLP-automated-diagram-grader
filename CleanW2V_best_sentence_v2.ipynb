{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Simple Sentence Similarity (from github) - with application of the methods to a sample dataset at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings have become widespread in Natural Language Processing. They allow us to easily compute the semantic similarity between two words, or to find the words most similar to a target word. However, in many applications we're more interested in the similarity between two sentences or short texts. In this notebook, I compare some simple ways of computing sentence similarity and investigate how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "First we need to do some preparation: some of our models require the sentences to be tokenized, some do not. For that reason we'll make a simple Sentence class where we keep both the raw sentence and the tokenized sentence. The individual methods below will then pick the input they need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "STOP = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "class Sentence:\n",
    "    \n",
    "    def __init__(self, sentence):\n",
    "        self.raw = sentence\n",
    "        normalized_sentence = sentence.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        self.tokens = [t.lower() for t in nltk.word_tokenize(normalized_sentence)]\n",
    "        self.tokens_without_stop = [t for t in self.tokens if t not in STOP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to use the popular [Gensim](https://radimrehurek.com/gensim/) library to load aset of widely used pre-trained word embeddings: \n",
    "[word2vec](https://www.tensorflow.org/tutorials/word2vec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "PATH_TO_WORD2VEC = os.path.expanduser(\"models/GoogleNews-vectors-negative300.bin\")\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(PATH_TO_WORD2VEC, binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in order to compute weighted averages of word embeddings later, we are going to load a file with word frequencies. These word frequencies have been collected from Wikipedia and saved in a tab-separated file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "PATH_TO_FREQUENCIES_FILE = \"data/sentence_similarity/frequencies.tsv\"\n",
    "PATH_TO_DOC_FREQUENCIES_FILE = \"data/sentence_similarity/doc_frequencies.tsv\"\n",
    "\n",
    "def read_tsv(f):\n",
    "    frequencies = {}\n",
    "    with open(f) as tsv:\n",
    "        tsv_reader = csv.reader(tsv, delimiter=\"\\t\")\n",
    "        for row in tsv_reader: \n",
    "            frequencies[row[0]] = int(row[1])\n",
    "        \n",
    "    return frequencies\n",
    "        \n",
    "frequencies = read_tsv(PATH_TO_FREQUENCIES_FILE)\n",
    "doc_frequencies = read_tsv(PATH_TO_DOC_FREQUENCIES_FILE)\n",
    "doc_frequencies[\"NUM_DOCS\"] = 1288431\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "As our baseline, we're going for the simplest way of computing sentence embeddings: just take the embeddings of the words in the sentence (minus the stopwords), and compute their average, weighted by the sentence frequency of each word. \n",
    "\n",
    "We then use the cosine to calculate the similarity between two sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None): \n",
    "\n",
    "    if doc_freqs is not None:\n",
    "        N = doc_freqs[\"NUM_DOCS\"]\n",
    "    \n",
    "    sims = []\n",
    "    for (sent1, sent2) in zip(sentences1, sentences2):\n",
    "    \n",
    "        tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens\n",
    "        tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens\n",
    "\n",
    "        tokens1 = [token for token in tokens1 if token in model]\n",
    "        tokens2 = [token for token in tokens2 if token in model]\n",
    "        \n",
    "        if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "            sims.append(0)\n",
    "            continue\n",
    "        \n",
    "        tokfreqs1 = Counter(tokens1)\n",
    "        tokfreqs2 = Counter(tokens2)\n",
    "        \n",
    "        weights1 = [tokfreqs1[token] * math.log(N/(doc_freqs.get(token, 0)+1)) \n",
    "                    for token in tokfreqs1] if doc_freqs else None\n",
    "        weights2 = [tokfreqs2[token] * math.log(N/(doc_freqs.get(token, 0)+1)) \n",
    "                    for token in tokfreqs2] if doc_freqs else None\n",
    "        \n",
    "        if weights1 and weights2:\n",
    "            if sum(weights1) == 0 or sum(weights2) == 0:\n",
    "                sims.append(0)\n",
    "                continue\n",
    "        embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1)\n",
    "        embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1)\n",
    "\n",
    "        sim = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "        sims.append(sim)\n",
    "\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Mover's Distance\n",
    "\n",
    "Word mover's distance is a popular alternative to the simple average embedding similarity. The Word Mover's Distance uses the word embeddings of the words in two texts to measure the minimum amount that the words in one text need to \"travel\" in semantic space to reach the words of the other text. Word mover's distance is available in the popular Gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wmd_benchmark(sentences1, sentences2, model, use_stoplist=False):\n",
    "    \n",
    "    sims = []\n",
    "    for (sent1, sent2) in zip(sentences1, sentences2):\n",
    "    \n",
    "        tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens\n",
    "        tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens\n",
    "        \n",
    "        tokens1 = [token for token in tokens1 if token in model]\n",
    "        tokens2 = [token for token in tokens2 if token in model]\n",
    "        \n",
    "        if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "            tokens1 = [token for token in sent1.tokens if token in model]\n",
    "            tokens2 = [token for token in sent2.tokens if token in model]\n",
    "            \n",
    "        sims.append(-model.wmdistance(tokens1, tokens2))\n",
    "        \n",
    "    return sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Inverse Frequency\n",
    "\n",
    "Taking the average of the word embeddings in a sentence, like we did above, is a very crude method of computing sentence embeddings. Most importantly, this gives far too much weight to words that are quite irrelevant, semantically speaking. Smooth Inverse Frequency tries to solve this problem. \n",
    "\n",
    "To compute SIF sentence embeddings, we first compute a weighted average of the token embeddings in the sentence. This procedure is very similar to the weighted average we used above, with the single difference that the word embeddings are weighted by `a/a+p(w)`, where `w` is a parameter that is set to `0.001` by default, and `p(w)` is the estimated relative frequency of a word in a reference corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to perform common component removal: we compute the principal component of the sentence embeddings we obtained above and subtract from them their projections on this first principal component. This corrects for the influence of high-frequency words that mostly have a syntactic or discourse function, such as \"just\", \"there\", \"but\", etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def remove_first_principal_component(X):\n",
    "    svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    pc = svd.components_\n",
    "    XX = X - X.dot(pc.transpose()) * pc\n",
    "    return XX\n",
    "\n",
    "\n",
    "def run_sif_benchmark(sentences1, sentences2, model, freqs={}, use_stoplist=False, a=0.001): \n",
    "    total_freq = sum(freqs.values())\n",
    "    embeddings = []\n",
    "    \n",
    "    # SIF requires us to first collect all sentence embeddings and then perform \n",
    "    # common component analysis.\n",
    "    for (sent1, sent2) in zip(sentences1, sentences2): \n",
    "        \n",
    "        tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens\n",
    "        tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens   \n",
    "        \n",
    "        tokens1 = [token for token in tokens1 if token in model]\n",
    "        tokens2 = [token for token in tokens2 if token in model]\n",
    "        \n",
    "        if tokens1 == []: tokens1 = ['empty']\n",
    "        if tokens2 == []: tokens2 = ['empty'] \n",
    "        \n",
    "        weights1 = [a/(a+freqs.get(token,0)/total_freq) for token in tokens1]\n",
    "        weights2 = [a/(a+freqs.get(token,0)/total_freq) for token in tokens2]\n",
    "        \n",
    "        embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=weights1)\n",
    "        embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=weights2)\n",
    "        embeddings.append(embedding1)\n",
    "        embeddings.append(embedding2)\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "    embeddings = remove_first_principal_component(np.array(embeddings))\n",
    "    sims = [cosine_similarity(embeddings[idx*2].reshape(1, -1), \n",
    "                              embeddings[idx*2+1].reshape(1, -1))[0][0] \n",
    "            for idx in range(int(len(embeddings)/2))]\n",
    "\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods above share two important characteristics: \n",
    "\n",
    "- As simple bag-of-word methods, they do take not word order into account.\n",
    "- The word embeddings they use have been learned in an unsupervised manner. \n",
    "\n",
    "Both these characteristics are potential downsides: \n",
    "\n",
    "- Since differences in word order can point to differences in meaning (compare `the dog bites the man` with `the man bites the dog`), we'd like our sentence embeddings to be sensitive to this variation.\n",
    "- Supervised training can help sentence embeddings learn the meaning of a sentence more directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(df, benchmarks): \n",
    "    \n",
    "    sentences1 = [Sentence(s) for s in df['sent_1']]\n",
    "    sentences2 = [Sentence(s) for s in df['sent_2']]\n",
    "    \n",
    "    pearson_cors, spearman_cors = [], []\n",
    "    for label, method in benchmarks:\n",
    "        sims = method(sentences1, sentences2)\n",
    "        pearson_correlation = scipy.stats.pearsonr(sims, df['sim'])[0]\n",
    "        print(label, pearson_correlation)\n",
    "        pearson_cors.append(pearson_correlation)\n",
    "        spearman_correlation = scipy.stats.spearmanr(sims, df['sim'])[0]\n",
    "        spearman_cors.append(spearman_correlation)\n",
    "        \n",
    "    return pearson_cors, spearman_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "\n",
    "benchmarks = [(\"AVG-W2V\", ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=False)),\n",
    "              (\"AVG-W2V-STOP\", ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=True)),\n",
    "              (\"AVG-W2V-TFIDF\", ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=False, doc_freqs=doc_frequencies)),\n",
    "              (\"AVG-W2V-TFIDF-STOP\", ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=True, doc_freqs=doc_frequencies)),\n",
    "              (\"WMD-W2V\", ft.partial(run_wmd_benchmark, model=word2vec, use_stoplist=False)), \n",
    "              (\"SIF-W2V\", ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=False)),\n",
    "             ]\n",
    "\n",
    "global PYEMD_EXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to our dataset - simple version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('Example_dataset_marble - 5_column_with_correct.csv')\n",
    "data = pd.read_csv('Example_dataset_marble_v2 - 2_data_no_omission.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple version\n",
    "unavailable for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_simple(df, benchmarks): \n",
    "    sentences1 = [Sentence(s) for s in df['Field_en']]\n",
    "    sentences2 = [Sentence(s) for s in df['Field_correct_en']]\n",
    "    sims = {\"Sent1\":sentences1, \"Sent2\": sentences2}\n",
    "    pearson_cors, spearman_cors = [], []\n",
    "    for label, method in benchmarks:\n",
    "        sims[label] = method(sentences1, sentences2)\n",
    "\n",
    "    frame = pd.DataFrame(sims)\n",
    "    frame['Field_en'] = df['Field_en']\n",
    "    frame['Field_correct_en'] = df['Field_correct_en']\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_frame = data.copy()\n",
    "exp_frame.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_frame[exp_frame['Field_en'].str.strip() == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_sim = run_all_simple(exp_frame, benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_sim = pd.concat([frame_sim, \n",
    "                       exp_frame[[\"Accuracy_score\",\"Code\",\n",
    "                                  \"Fieldname\"]]], \n",
    "                        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_sim = frame_sim[['Fieldname', \"Field_en\",'Field_correct_en',\n",
    "                       'AVG-W2V', 'AVG-W2V-STOP', 'AVG-W2V-TFIDF', \n",
    "                       'AVG-W2V-TFIDF-STOP','WMD-W2V','SIF-W2V', \n",
    "                       'Accuracy_score','Code']]\n",
    "frame_sim.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_match(df, model, benchmarks): \n",
    "    size = len(model.index)\n",
    "    text_frame = df.copy()\n",
    "    sims = {\"stud_sentence\":[],\n",
    "            \"stud_field\":[],}\n",
    "    for label, method in benchmarks:\n",
    "        sims[label+\"_all_scores\"] = []\n",
    "        sims[label+\"_similarity\"] = []\n",
    "        sims[label+\"_aimed_sentence\"] = []\n",
    "        sims[label+\"_aimed_field\"] = []\n",
    "        \n",
    "    for index, row in text_frame.iterrows():\n",
    "        stud_sentence = row[\"Field_en\"]\n",
    "        sims[\"stud_sentence\"].append(stud_sentence)\n",
    "        sims[\"stud_field\"].append(row[\"Fieldname\"])\n",
    "        student_sentences = [Sentence(stud_sentence)]*size\n",
    "        model_sentences = model[row['Category']].apply(lambda s: Sentence(s))\n",
    "    #   pearson_cors, spearman_cors = [], []\n",
    "        for label, method in benchmarks:\n",
    "            similarity_scores = method(student_sentences, model_sentences)\n",
    "            similarity = max(similarity_scores)\n",
    "            index = np.argmax(similarity_scores)\n",
    "            aimed_sentence = model_sentences.iloc[index]\n",
    "            aimed_field = model_sentences.index[index]\n",
    "            sims[label+\"_all_scores\"].append(similarity_scores)\n",
    "            sims[label+\"_similarity\"].append(similarity)\n",
    "            sims[label+\"_aimed_sentence\"].append(aimed_sentence.raw)\n",
    "            sims[label+\"_aimed_field\"].append(aimed_field)\n",
    "    frame = pd.DataFrame(sims)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>TextName</th>\n",
       "      <th>Beton</th>\n",
       "      <th>Botox</th>\n",
       "      <th>Geld</th>\n",
       "      <th>Metro</th>\n",
       "      <th>Muziek</th>\n",
       "      <th>Suez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Field1_en</th>\n",
       "      <td>Central heating</td>\n",
       "      <td>Can help prevent muscle tightness</td>\n",
       "      <td>People should not be distracted for too long risk</td>\n",
       "      <td>Waste need not be processed in some way</td>\n",
       "      <td>learn to read and play music</td>\n",
       "      <td>No natural connection Western Indian Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Field2_en</th>\n",
       "      <td>Concrete dries out</td>\n",
       "      <td>Can help against wrinkles between the eyes and...</td>\n",
       "      <td>A person may experience only short deep happin...</td>\n",
       "      <td>Artificial reef constructed</td>\n",
       "      <td>Improving mathematics vaardigheiten</td>\n",
       "      <td>Ships make long trip around African continent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Field3_en</th>\n",
       "      <td>Buildings are smaller</td>\n",
       "      <td>People look younger</td>\n",
       "      <td>People get used to luxury</td>\n",
       "      <td>More plankton and marine fauna</td>\n",
       "      <td>Can help bring back old memories</td>\n",
       "      <td>Shorter waterway needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Field4_en</th>\n",
       "      <td>Elevators Bliven hang</td>\n",
       "      <td>Facial expression can change</td>\n",
       "      <td>Money does not gellukig long time</td>\n",
       "      <td>More fish (such as mackerel, grouper, sea fish...</td>\n",
       "      <td>Higher scores on IQ tests</td>\n",
       "      <td>Suez canal dug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "TextName                   Beton  \\\n",
       "Field1_en        Central heating   \n",
       "Field2_en     Concrete dries out   \n",
       "Field3_en  Buildings are smaller   \n",
       "Field4_en  Elevators Bliven hang   \n",
       "\n",
       "TextName                                               Botox  \\\n",
       "Field1_en                  Can help prevent muscle tightness   \n",
       "Field2_en  Can help against wrinkles between the eyes and...   \n",
       "Field3_en                                People look younger   \n",
       "Field4_en                       Facial expression can change   \n",
       "\n",
       "TextName                                                Geld  \\\n",
       "Field1_en  People should not be distracted for too long risk   \n",
       "Field2_en  A person may experience only short deep happin...   \n",
       "Field3_en                          People get used to luxury   \n",
       "Field4_en                  Money does not gellukig long time   \n",
       "\n",
       "TextName                                               Metro  \\\n",
       "Field1_en            Waste need not be processed in some way   \n",
       "Field2_en                        Artificial reef constructed   \n",
       "Field3_en                     More plankton and marine fauna   \n",
       "Field4_en  More fish (such as mackerel, grouper, sea fish...   \n",
       "\n",
       "TextName                                Muziek  \\\n",
       "Field1_en         learn to read and play music   \n",
       "Field2_en  Improving mathematics vaardigheiten   \n",
       "Field3_en     Can help bring back old memories   \n",
       "Field4_en            Higher scores on IQ tests   \n",
       "\n",
       "TextName                                            Suez  \n",
       "Field1_en     No natural connection Western Indian Ocean  \n",
       "Field2_en  Ships make long trip around African continent  \n",
       "Field3_en                        Shorter waterway needed  \n",
       "Field4_en                                 Suez canal dug  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_frame = pd.read_csv('correct_answers.csv', index_col=0)\n",
    "exp_frame2 = data.copy()\n",
    "# For old dataset\n",
    "# exp_frame2['Category'] = [tab[0] for tab in exp_frame2['Fieldname'].str.split(\";\")]\n",
    "model_frame = model_frame[[ 'Field1_en', 'Field2_en','Field3_en', 'Field4_en']]\n",
    "model_frame = model_frame.transpose()\n",
    "model_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_sim2 = run_all_match(exp_frame2, model_frame, benchmarks)\n",
    "frame_sim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_sim2 = pd.concat([frame_sim2, \n",
    "                        exp_frame2[['IDStud', 'IDClass', \n",
    "                                    \"Category\",\"Accuracy_score\",\n",
    "                                    \"Code\",\"Fieldname\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_sim2.to_csv(\"complete_result_matched.csv\")\n",
    "sif_matched = frame_sim2[[\"SIF-W2V_aimed_sentence\", \n",
    "            \"SIF-W2V_aimed_field\", \n",
    "            'SIF-W2V_similarity',\n",
    "            'stud_field',\n",
    "            'stud_sentence',\n",
    "            'IDStud', 'IDClass', \n",
    "            \"Category\",\"Accuracy_score\",\n",
    "            \"Code\",\"Fieldname\"]].copy()\n",
    "sif_matched.to_csv(\"sif_wv2_matched.csv\")\n",
    "avg_w2v_matched = frame_sim2[[\"AVG-W2V_aimed_sentence\", \n",
    "            \"AVG-W2V_aimed_field\", \n",
    "            'AVG-W2V_similarity',\n",
    "            'stud_field',\n",
    "            'stud_sentence',\n",
    "            'IDStud', 'IDClass', \n",
    "            \"Category\",\"Accuracy_score\",\n",
    "            \"Code\",\"Fieldname\"]].copy()\n",
    "avg_w2v_matched.to_csv(\"avg_wv2_matched.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
